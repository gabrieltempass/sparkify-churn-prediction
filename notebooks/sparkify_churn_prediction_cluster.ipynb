{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Clean Dataset\n",
    "Loading and cleaning the dataset, checking for invalid or missing data - for example, records without user IDs or session IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Importing libraries and setting up notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, udf, from_unixtime, year, weekofyear, substring, encode, decode, split, desc, avg, first, concat_ws, countDistinct, sum as Fsum, max as Fmax, min as Fmin\n",
    "from pyspark.sql.types import StringType, LongType, IntegerType, DateType, TimestampType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, PCA\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "\n",
    "from user_agents import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('SparkifyCluster') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read sparkify dataset\n",
    "\n",
    "filepath = 'mini_sparkify_event_data.json'\n",
    "df_log = spark.read.json(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_log shape: (286500, 18)\n"
     ]
    }
   ],
   "source": [
    "print('df_log shape:', (df_log.count(), len(df_log.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Adjusting data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the data types\n",
    "\n",
    "df_log = df_log \\\n",
    "    .withColumn('registration', from_unixtime(col('registration')/1000).cast(TimestampType())) \\\n",
    "    .withColumn('status', col('status').cast(StringType())) \\\n",
    "    .withColumn('ts', from_unixtime(col('ts')/1000).cast(TimestampType())) \\\n",
    "    .withColumn('userId', col('userId').cast(LongType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Removing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_log shape:\t\t (286500, 18)\n",
      "df_log_valid shape:\t (278154, 18)\n",
      "8346 rows with empty user and session IDs removed\n"
     ]
    }
   ],
   "source": [
    "df_log_valid = df_log.dropna(how='any', subset=['userId', 'sessionId'])\n",
    "\n",
    "old_rows, old_columns = df_log.count(), df_log.columns\n",
    "new_rows, new_columns = df_log_valid.count(), df_log_valid.columns\n",
    "\n",
    "print('df_log shape:\\t\\t', (old_rows, len(old_columns)))\n",
    "print('df_log_valid shape:\\t', (new_rows, len(new_columns)))\n",
    "print('{} rows with empty user and session IDs removed'.format(old_rows - new_rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Correcting encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String should be:\t Björk\n",
      "String currently is:\t BjÃÂ¶rk\n"
     ]
    }
   ],
   "source": [
    "# Show example of string with wrong encoding\n",
    "\n",
    "record = '''userId == 30\n",
    "            and sessionId == 29\n",
    "            and itemInSession == 68'''\n",
    "\n",
    "encoding_example = df_log_valid.where(record).collect()[0][0]\n",
    "\n",
    "print('String should be:\\t Björk')\n",
    "print('String currently is:\\t {}'.format(encoding_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode_column(df, column, encoding, decoding):\n",
    "    \"\"\"Encode a column from a dataframe and then decode it.\n",
    "    \n",
    "    Parameters:\n",
    "        df (Spark dataframe): The dataframe that contains the column.\n",
    "        column (String): The name of the column to be encoded and decoded.\n",
    "        encoding (String): The charset of the encoding (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
    "        decoding (String): The charset of the decoding (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
    "    \n",
    "    Returns:\n",
    "        df (Spark dataframe): The dataframe with the column properly encoded and decoded\n",
    "        \n",
    "    Example:\n",
    "        df = encode_decode_column(df, 'column_name', 'ISO-8859-1', 'UTF-8')\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.withColumn(column, encode(column, encoding))\n",
    "    df = df.withColumn(column, decode(column, decoding))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = list(filter(lambda c: c[1] == 'string', df_log_valid.dtypes))\n",
    "categorical_columns = [item[0] for item in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the wrong encoding for the columns that are strings,\n",
    "# and in order to retrieve the correct characters the encode-decode process must be done twice.\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df_log_valid = encode_decode_column(df_log_valid, column, 'ISO-8859-1', 'UTF-8')\n",
    "    df_log_valid = encode_decode_column(df_log_valid, column, 'ISO-8859-1', 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String should be:\t Björk\n",
      "String currently is:\t Björk\n"
     ]
    }
   ],
   "source": [
    "# Test if the previous example is correct now\n",
    "\n",
    "encoding_example_fixed = df_log_valid.where(record).collect()[0][0]\n",
    "\n",
    "print('String should be:\\t Björk')\n",
    "print('String currently is:\\t {}'.format(encoding_example_fixed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Parsing user agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parsing functions with the user_agents library\n",
    "\n",
    "get_browser     = udf(lambda x: parse(x).browser.family, StringType())\n",
    "get_os          = udf(lambda x: parse(x).os.family, StringType())\n",
    "get_device      = udf(lambda x: parse(x).device.family, StringType())\n",
    "get_is_phone    = udf(lambda x: 1 if parse(x).is_mobile else 0, IntegerType())\n",
    "get_is_tablet   = udf(lambda x: 1 if parse(x).is_tablet else 0, IntegerType())\n",
    "get_is_computer = udf(lambda x: 1 if parse(x).is_pc else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_valid = df_log_valid \\\n",
    "    .withColumn('browser', get_browser('userAgent')) \\\n",
    "    .withColumn('os', get_os('userAgent')) \\\n",
    "    .withColumn('device', get_device('userAgent')) \\\n",
    "    .withColumn('isPhone', get_is_phone('userAgent')) \\\n",
    "    .withColumn('isTablet', get_is_tablet('userAgent')) \\\n",
    "    .withColumn('isComputer', get_is_computer('userAgent'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Main stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct user IDs: 225\n"
     ]
    }
   ],
   "source": [
    "distinct_user_ids = df_log_valid.dropDuplicates(['userId']).count()\n",
    "print('Distinct user IDs:', distinct_user_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Defining churn\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag the churn\n",
    "\n",
    "flag_cancellation_event = udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())\n",
    "df_log_valid = df_log_valid.withColumn('churn', flag_cancellation_event('page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_val = Window.partitionBy('userId').orderBy(desc('ts')).rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df_log_valid = df_log_valid.withColumn('churned', Fsum('churn').over(window_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|churned|count|\n",
      "+-------+-----+\n",
      "|      0|  173|\n",
      "|      1|   52|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_log_valid \\\n",
    "    .select('userId', 'churned') \\\n",
    "    .dropDuplicates(['userId']) \\\n",
    "    .groupBy('churned') \\\n",
    "    .count() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cohort\n",
    "\n",
    "df_log_valid = df_log_valid.withColumn('cohort', substring('registration', 1, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Building users features matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_features = df_log_valid.select('userId', 'churned').dropDuplicates(['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create year and week columns to aggregate data in the next steps\n",
    "\n",
    "df_log_valid = df_log_valid \\\n",
    "    .withColumn('year', year(col('ts').cast(DateType()))) \\\n",
    "    .withColumn('week', weekofyear(col('ts').cast(DateType()))) \\\n",
    "    .withColumn('yearWeek', concat_ws('-', col('year'), col('week')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the avg. songs per week\n",
    "\n",
    "df_avg_songs_week = df_log_valid \\\n",
    "    .where('page = \"NextSong\"') \\\n",
    "    .groupby('userId','yearWeek') \\\n",
    "    .count() \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(avg('count').alias('avgSongsWeek'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the avg. sessions per week\n",
    "\n",
    "df_avg_sessions_week = df_log_valid \\\n",
    "    .groupby('userId','yearWeek') \\\n",
    "    .agg(countDistinct('sessionId').alias('sessions')) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(avg('sessions').alias('avgSessionsWeek'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the avg. session duration\n",
    "\n",
    "df_avg_session_duration = df_log_valid \\\n",
    "    .groupby('userId','sessionId') \\\n",
    "    .agg(Fmin('ts').alias('start'), Fmax('ts').alias('end')) \\\n",
    "    .withColumn('sessionDuration', col('end').cast(LongType()) - col('start').cast(LongType())) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(avg('sessionDuration').alias('avgSessionDuration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cohort\n",
    "\n",
    "df_cohort = df_log_valid \\\n",
    "    .withColumn('cohort', substring('registration', 1, 7)) \\\n",
    "    .select('userId', 'cohort') \\\n",
    "    .dropDuplicates(['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the average song length\n",
    "\n",
    "df_length = df_log_valid \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(avg('length').alias('length')) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the most recent metropolitan area\n",
    "\n",
    "df_metro_area = df_log_valid \\\n",
    "    .withColumn('metropolitanArea', split('location', ',')[0]) \\\n",
    "    .orderBy(desc('ts')) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(first('metropolitanArea').alias('metropolitanArea'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the most recent state\n",
    "\n",
    "df_state = df_log_valid \\\n",
    "    .withColumn('state', split('location', ',')[1]) \\\n",
    "    .orderBy(desc('ts')) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(first('state').alias('state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the most recent gender\n",
    "\n",
    "df_gender = df_log_valid \\\n",
    "    .orderBy(desc('ts')) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(first('gender').alias('gender'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the most recent level\n",
    "\n",
    "df_level = df_log_valid \\\n",
    "    .orderBy(desc('ts')) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(first('level').alias('level'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the most used browser\n",
    "\n",
    "df_browser = df_log_valid \\\n",
    "    .select('userId', 'browser') \\\n",
    "    .groupBy('userId', 'browser') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count')) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(first('browser').alias('browser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the most used OS\n",
    "\n",
    "df_os = df_log_valid \\\n",
    "    .select('userId', 'os') \\\n",
    "    .groupBy('userId', 'os') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count')) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(first('os').alias('os'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the most used device\n",
    "\n",
    "df_device = df_log_valid \\\n",
    "    .select('userId', 'device') \\\n",
    "    .groupBy('userId', 'device') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count')) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(first('device').alias('device'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the most used is phone\n",
    "\n",
    "df_is_phone = df_log_valid \\\n",
    "    .select('userId', 'isPhone') \\\n",
    "    .groupBy('userId', 'isPhone') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count')) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(first('isPhone').alias('isPhone'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the most used is tablet\n",
    "\n",
    "df_is_tablet = df_log_valid \\\n",
    "    .select('userId', 'isTablet') \\\n",
    "    .groupBy('userId', 'isTablet') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count')) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(first('isTablet').alias('isTablet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the most used is computer\n",
    "\n",
    "df_is_computer = df_log_valid \\\n",
    "    .select('userId', 'isComputer') \\\n",
    "    .groupBy('userId', 'isComputer') \\\n",
    "    .count() \\\n",
    "    .orderBy(desc('count')) \\\n",
    "    .groupBy('userId') \\\n",
    "    .agg(first('isComputer').alias('isComputer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_features = df_users_features \\\n",
    "    .join(df_avg_songs_week, on='userId') \\\n",
    "    .join(df_avg_sessions_week, on='userId') \\\n",
    "    .join(df_avg_session_duration, on='userId') \\\n",
    "    .join(df_cohort, on='userId') \\\n",
    "    .join(df_length, on='userId') \\\n",
    "    .join(df_metro_area, on='userId') \\\n",
    "    .join(df_state, on='userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_features = df_users_features \\\n",
    "    .join(df_gender, on='userId') \\\n",
    "    .join(df_level, on='userId') \\\n",
    "    .join(df_browser, on='userId') \\\n",
    "    .join(df_os, on='userId') \\\n",
    "    .join(df_device, on='userId') \\\n",
    "    .join(df_is_phone, on='userId') \\\n",
    "    .join(df_is_tablet, on='userId') \\\n",
    "    .join(df_is_computer, on='userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Processing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector    = 'features'\n",
    "target_vector     = 'label'\n",
    "prediction_vector = 'prediction'\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_users_features \\\n",
    "    .drop('userId', 'metropolitanArea', 'state') \\\n",
    "    .withColumnRenamed('churned', target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = df_users_features \\\n",
    "    .drop('userId', 'metropolitanArea', 'state') \\\n",
    "    .withColumnRenamed('churned', target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_level   = StringIndexer(inputCol='level',   outputCol='levelIndex')\n",
    "indexer_gender  = StringIndexer(inputCol='gender',  outputCol='genderIndex')\n",
    "indexer_cohort  = StringIndexer(inputCol='cohort',  outputCol='cohortIndex')\n",
    "indexer_browser = StringIndexer(inputCol='browser', outputCol='browserIndex')\n",
    "indexer_device  = StringIndexer(inputCol='device',  outputCol='deviceIndex')\n",
    "indexer_os      = StringIndexer(inputCol='os',      outputCol='osIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_inputs  = ['levelIndex', 'genderIndex', 'cohortIndex', 'browserIndex', 'deviceIndex', 'osIndex']\n",
    "ohe_outputs = ['levelOhe',   'genderOhe',   'cohortOhe',   'browserOhe',   'deviceOhe',   'osOhe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoderEstimator(inputCols=ohe_inputs, outputCols=ohe_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "va_inputs = ['avgSongsWeek',\n",
    "             'avgSessionsWeek',\n",
    "             'avgSessionDuration',\n",
    "             'length',\n",
    "             'isPhone',\n",
    "             'isTablet',\n",
    "             'isComputer',\n",
    "             'levelOhe',\n",
    "             'genderOhe',\n",
    "             'cohortOhe',\n",
    "             'browserOhe',\n",
    "             'deviceOhe',\n",
    "             'osOhe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=va_inputs, outputCol=feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_process = Pipeline(stages=[\n",
    "    indexer_level,\n",
    "    indexer_gender,\n",
    "    indexer_cohort,\n",
    "    indexer_browser,\n",
    "    indexer_device,\n",
    "    indexer_os,\n",
    "    one_hot_encoder,\n",
    "    vector_assembler\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.37 s, sys: 1.66 s, total: 4.04 s\n",
      "Wall time: 5min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_processed = pipeline_process.fit(df_processed).transform(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Establishing a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df_processed.randomSplit((0.8, 0.2), seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Tunning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol=feature_vector, labelCol=target_vector, predictionCol=prediction_vector, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Pipeline(stages=[rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder().addGrid(rf.numTrees, [20]) \\\n",
    "                         .addGrid(rf.maxDepth, [5]) \\\n",
    "                         .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'f1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=prediction_vector, labelCol=target_vector, metricName=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=estimator, estimatorParamMaps=grid, evaluator=evaluator, numFolds=3, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.12 s, sys: 2.69 s, total: 6.8 s\n",
      "Wall time: 13min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = cv.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score on the test set is: 83.1727%\n",
      "Best parameters: maxDepth: 5, numTrees: 20\n",
      "CPU times: user 521 ms, sys: 342 ms, total: 862 ms\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "score = evaluator.evaluate(model.transform(df_test))\n",
    "\n",
    "best_model = model.bestModel\n",
    "max_depth = best_model.stages[0].getOrDefault('maxDepth')\n",
    "num_trees = best_model.stages[0].getOrDefault('numTrees')\n",
    "\n",
    "print('The {} score on the test set is: {:.4%}'.format(metric.upper(), score))\n",
    "print('Best parameters: maxDepth: {}, numTrees: {}'.format(max_depth, num_trees))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "84.5029% - just with userAgent, gender and cohort\n",
    "83.1727% - adding avgSongsWeek, avgSessionsWeek, avgSessionDuration, length and level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_importances(best_model, features, decimal=4):\n",
    "\n",
    "    scores = list(best_model.stages[0].featureImportances)\n",
    "    \n",
    "    # Sort features according to the highest score\n",
    "    zipped_lists = zip(scores, features)\n",
    "    sorted_zipped_lists = sorted(zipped_lists, reverse=True)\n",
    "    unzipped_list = list(zip(*sorted_zipped_lists))\n",
    "    scores, features = list(unzipped_list[0]), list(unzipped_list[1])\n",
    "    \n",
    "    # Define the name of the columns for the table header\n",
    "    header_1_name = 'feature'\n",
    "    header_2_name = 'importance'\n",
    "    \n",
    "    # Garantee the width of the columns are at least the same width of the header names\n",
    "    width = len(max(features, key=len))\n",
    "    if width < len(header_1_name):\n",
    "        border_header_1 = len(header_1_name)\n",
    "    else:\n",
    "        border_header_1 = width\n",
    "    \n",
    "    if len('0.') + decimal < len(header_2_name):\n",
    "        border_header_2 = len(header_2_name)\n",
    "    else:\n",
    "        border_header_2 = len('0.') + decimal\n",
    "    \n",
    "    # Number of characters to left align the strings\n",
    "    align = border_header_2\n",
    "    \n",
    "    # Calculate number of spaces needed after the header name\n",
    "    header_1_spaces = width - len(header_1_name)\n",
    "    header_2_spaces = (len('0.') + decimal) - len(header_2_name)\n",
    "    \n",
    "    # Build border and header strings according to the width of the header names, features and decimal places\n",
    "    border = '+' + '-'*border_header_1 + '+' + '-'*border_header_2 + '+'\n",
    "    header = '|' + header_1_name + ' '*header_1_spaces + '|' + header_2_name + ' '*header_2_spaces + '|'\n",
    "    \n",
    "    # Print results in a fancy table\n",
    "    print(border)\n",
    "    print(header)\n",
    "    print(border)\n",
    "    for i in range(len(features)):\n",
    "        print('|{:{width}}|{:<{align}.{decimal}f}|'.format(features[i], scores[i], width=width, align=align, decimal=decimal))\n",
    "    print(border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [attr['name'] for attr in (chain(*df_processed.schema[feature_vector].metadata['ml_attr']['attrs'].values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------+\n",
      "|feature                 |importance|\n",
      "+------------------------+----------+\n",
      "|length                  |0.2065    |\n",
      "|avgSessionsWeek         |0.1813    |\n",
      "|avgSessionDuration      |0.1505    |\n",
      "|avgSongsWeek            |0.1490    |\n",
      "|genderOhe_M             |0.0531    |\n",
      "|browserOhe_Firefox      |0.0396    |\n",
      "|levelOhe_paid           |0.0362    |\n",
      "|deviceOhe_Mac           |0.0255    |\n",
      "|cohortOhe_2018-08       |0.0234    |\n",
      "|browserOhe_Chrome       |0.0225    |\n",
      "|cohortOhe_2018-07       |0.0169    |\n",
      "|osOhe_Windows           |0.0140    |\n",
      "|osOhe_iOS               |0.0139    |\n",
      "|cohortOhe_2018-09       |0.0136    |\n",
      "|deviceOhe_Other         |0.0108    |\n",
      "|browserOhe_Safari       |0.0107    |\n",
      "|browserOhe_Mobile Safari|0.0082    |\n",
      "|cohortOhe_2018-06       |0.0080    |\n",
      "|osOhe_Linux             |0.0057    |\n",
      "|osOhe_Mac OS X          |0.0037    |\n",
      "|cohortOhe_2018-11       |0.0034    |\n",
      "|isComputer              |0.0024    |\n",
      "|deviceOhe_iPhone        |0.0009    |\n",
      "|isPhone                 |0.0003    |\n",
      "|isTablet                |0.0000    |\n",
      "|cohortOhe_2018-05       |0.0000    |\n",
      "|cohortOhe_2018-03       |0.0000    |\n",
      "+------------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "print_feature_importances(best_model, features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
